%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl}

\usepackage{times}
\usepackage{amsmath}
\usepackage{url}
\usepackage{latexsym}
\usepackage{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{makecell}
\usepackage[utf8]{inputenc}
\usepackage[russian,english]{babel}
\usepackage{geometry}
\usepackage{pdflscape}
\usepackage{caption}
\usepackage{xcolor}
\usepackage{booktabs,tabularx}

%\usepackage{showframe}
\renewcommand{\UrlFont}{\ttfamily\small}
\newcommand{\argmax}{\mathrm{argmax}}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\title{Quantifying Morphology in Latent Dirichlet Analysis Topic Models}

\author{Virginia Partridge \\
  University of Massachusetts Amherst\\
  \texttt{vcpartridge@umass.edu}
}
\date{}

\begin{document}
\maketitle
\begin{abstract}
    Latent Dirichlet allocation (LDA) is a popular approach for probabilistic topic modeling, frequently applied in many disciplines for exploring themes and trends in large document collections. Because LDA assumes a bag-of-words approach, stemming or lemmatization are common pre-processing steps in preparing corpora for topic modeling, despite little evidence that these improve topic quality for English. Recent work has suggested post-processing topics so that they are more interpretable by end users may be a better approach.

    There is more motivation to apply stemming or lemmatization for topic modeling on languages with complex inflectional morphology, due to concerns that rare word forms will either be randomly assigned to topics or cause lexemes that are important to a topic's interpretability to be obscured.
    We present several metrics designed to quantify the morphological and lexical complexity of topics learned by LDA, with a focus on identifying topics that may benefit from post-processing. We then use these metrics to analyze LDA topic models trained with Gibbs sampling on Russian and German corpora, comparing the effects of different stemmers and lemmatizers.
    % TODO Summarize findings...
\end{abstract}

\section{Introduction}
Latent Dirichlet allocation (LDA) is a widely adopted approach for unsupervised topic modeling and has been used across disciplines for exploring themes and trends in large document collections. LDA has been applied to explore the ever-growing variety of text from online platforms and to analyze language changes in academic fields over time \cite{koltsova2013,mcfarland2013differentiating, vogel-jurafsky-2012-said, mitrofanova2015probabilistic}. Assuming a bag-of-words approach, LDA produces latent topics as multinomial distributions over words and each topic is viewed as being generated by a mixture of topics \cite{blei2003,steyvers2007probabilistic}.

However, what happens when words in this bag-of-words approach are themselves are complex? Stemming and lemmatization treatments are typical text preprocessing steps for topic modeling, even for English, which has relatively little inflectional morphology, but there is a lack of empirical evidence that these treatments improve the models from the perspective of human interpretability or quantitative measures of topic quality \cite{schofield-mimno-2016-comparing}. To understand the effects of these treatments on languages with more inflectional morphology, we train topic models on the German TIGER corpus\footnote{\url{https://www.ims.uni-stuttgart.de/en/research/resources/corpora/tiger/}} \cite{Brants2004TIGERLI} and the Russian National corpus\footnote{\url{https://ruscorpora.ru/old/en/corpora-morph.html}} (RNC) \cite{Apresjan2006ASA}. These corpora have high quality morphological and syntactic annotations, which allow for analysis based on gold standard morphological analyses and lemmatization.

Following Schofield and Mimno's work, we use Variation of information (VOI) to show how stemming and lemmatization change topic assignments under various pre-processing treatments. The stability of topic assignments under each treatment show how pre-processing affects experimental reproducibility. Additionally, we adapt measures of morphological complexity to analyzing LDA topics produced by Gibbs sampling in order to quantify the ways in which inflectional morphology influence topic models and identify topics where morphology most strongly complicates interpretability. So long as reliable morphological analyses are available, these methods can be applied cross-linguistically, which we demonstrate using topic models for TIGER and RNC. We also find evidence that stemming in pre-processing hurts topic stability, but


\section{Related Work}
The proposal for applying stemming in post-processing comes from work comparing the effects of various stemming approaches on English \cite{schofield-mimno-2016-comparing}. After comparing the relative strengths, qualitative and quantitative impacts of rule-based and context-based stemmers for English, it was concluded that stemming in pre-processing does not empirically improve LDA topic models and may hurt topic stability. Post-processing may still be valuable from the perspective of topic interpretability, avoiding repeating different surface forms of the same lexeme in topics' key word lists and presenting users with concise results.

Probabilistic topic modeling has been applied on Russian text data from academic fields, social media, and Wikipedia articles \cite{mitrofanova2015probabilistic,koltsova2013,May2016AnAO}. Prior to the work on Wikipedia, little attention was given to the role of lemmatization on topic modeling in Russian, and corpora were lemmatized by default. In studying Russian Wikipedia, May et al. (2016) address the impact of lemmatization on topic interpretability via a word intrusion evaluation task, finding that lemmatization may be beneficial. However, they also suggest measuring the effects of lemmatization and do not rule out that lemmatizing in post-processing would also be effective.

Although we found little prior work on the effects of stemming on topic modeling for German, Rieger et al. present methods for improving the stability of LDA and detail results of experiments on a German newspaper corpus \cite{Rieger2020ImprovingLD}. Their focus is on increasing the reproducibility of LDA topic models by choosing the initial token allocations for Gibbs sampling after comparing multiple LDA models and they limit pre-processing to removal of stopwords and punctuation. Schofield and Mimno also explicitly measured stability of token allocations by using VOI to compare the stemming treatments, finding that certain types of stemming could increase the impact of random initialization, hurting reproducibility.

% TODO Linguistic typology and measuring morphological complexity


\section{Background}

\subsection{Latent Dirichlet Analysis}
LDA uses the observed frequencies of vocabulary terms within documents to infer the \textit{latent}, or hidden, distributions of topics over words and topic assignments for each document. Once a number of topics $T$ is selected, the multinomial distributions $\phi_1,...\phi_T$ define the distribution of each topic $t$ over the vocabulary terms. Each $\phi_t$ is drawn from with a Dirichlet prior with concentration parameter $\beta$. Each document $d$ also has a multinomial distribution $\theta_d$ over the terms in the vocabulary, also drawn from a Dirichlet prior with concentration parameter $\alpha$. Viewing LDA as a generative process with a joint distribution of the observed and latent variables, find the $\phi_t$ and $\theta_d$ that maximize the likelihood of the corpus if you were to assign tokens to documents using the marginal distributions over topic assignments for the terms in each document. Gibbs Sampling allows estimation of the posterior for the joint topic distribution conditioned on the observed term frequencies by directly assigning topics to each token in the corpus, iteratively sampling topics and updating topic assignments
 \cite{steyvers2007probabilistic, blei2003,schofield-mimno-2016-comparing}.

Following Wallach et. al (2002), we will use a symmetric prior for $\beta$ and an asymmetric prior for $\alpha$ with the MALLET's Gibbs Sampling implementation to train topic models \cite{wallach2009rethinking,McCallumMALLET}. These parameters are optimized every 20 iterations after the first 50, the burn-in period. The Gibbs sampling implementation in MALLET allows us to directly inspect the topic assignments at the level of each token in a document.

\subsection{Framework for Morphological Complexity}
We will first clarify terms for discussing Russian's morphological paradigms, following frameworks for quantifying morphological complexity used in linguistics and computational linguistics \cite{baerman2015intro,Ackerman2013MorphologicalOT, cotterell-etal-2019-complexity}. We draw a distinction between \textit{derivational} morphology, the process by which new words are formed through changing meaning or part-of-speech, and \textit{inflectional} morphology, which can be simplistically understood as verb paradigms to capture subject-verb agreement or noun declensions for case and grammatical gender. For our purposes here, we are primarily interested in the equivalence classes formed by normalizing inflectional morphology, to use an English example, conflating ``respond" and ``responds", rather than ``respond" and ``responsiveness", although aggressive stemming methods will do both types of conflation.

In the word-based morphology framework, inflection is captured by triples consisting of the surface form (also called wordform) $w$, a lexeme  signifying the meaning and a slot $\sigma$, which can be understood as a set of ``atomic" units of morphological meaning, also called inflectional features \cite{aronoff1976word,sylak-glassman-etal-2015-language,cotterell-etal-2019-complexity}.
A lemma is the surface form used to look up the lexeme in a dictionary, such as the infinitive verb form. Measurements of the size of a lexeme's morphological paradigm capture \textit{enumerative complexity}, the number of distinct surface forms for a particular part-of-speech \cite{Ackerman2013MorphologicalOT}. A lexeme's mapping between slots and the surface forms is not always straightforward outside the context of a sentence, as multiple slots may be realized with a single surface form. This type of morphological complexity is called \textit{syncretism} and is common in Russian noun and adjective declensions \cite{baerman2015understanding,Milizia2015PatternsOS}. German also demonstrates syncretism in adjective agreement for noun case and gender \cite{Crysmann2005SyncretismIG}.

\section{Corpora}

\section{Methods}
% TODO: detail corpus preprocessing


\subsection{Stemmers and Lemmatization Treatments}
% TODO detail German stemmers (spacy and snowball)
\label{sec:stemmers}
Following Schofield and Mimno (2016), we distinguish between rule-based stemmers, which are deterministic, but only remove endings and do not map to lemmas, and context-based lemmatizers, which rely on a dictionary of word forms paired with outputs from a part-of-speech tagger to produce lemmas \cite{schofield-mimno-2016-comparing,Sharoff2011ThePP}. Rule-based methods make no distinction between inflectional and derivational morphological processes, leading to word types, conflation classes of terms, whose original surface forms may cover several lemmas.

\textbf{Oracle:}

\textbf{Truncation:} This simple baseline method trims surface forms to the first $n$ characters \cite{schofield-mimno-2016-comparing}. We truncate with $n=5$.

\textbf{Snowball Stemmer:} This stemmer was introduced as a rigorous framework for implementing stemming algorithms for a variety of languages. We utilize the NLTK implementation\footnote{\url{https://www.nltk.org/api/nltk.stem.html}} with the original rules for Russian\footnote{\url{http://snowball.tartarus.org/algorithms/russian/stemmer.html}} and German\footnote{\url{http://snowball.tartarus.org/algorithms/german/stemmer.html}} \cite{snowball}.


\textbf{Mystem:} This Yandex-owned tool is the most popular Russian lemmatizer and can be used without part-of-speech tags. Pairing a finite state machine algorithm for stemming with the Zalizniak grammatical dictionary for morphological tags, this system outputs a list of possible lemmas and slots for a given token input. The system also produces probabilities for each lemma and slot based on word frequency statistics, although the source corpus for these probabilities is not clear \cite{Segalovich2003AFM}. This is not truly a context-based lemmatizer, as it does not use part-of-speech tags to disambiguate between lemmas or to assign a single slot to a syncretic surface form, but the word frequencies do represent some kind of contextual prior. We use the python wrapper for Mystem, pymystem3\footnote{\url{pythonhosted.org/pymystem3/pymystem3.html}}. Notably, Mystem is as fast as the Snowball stemmer, while producing a normalized lemma form that is more interpretable for users.

\textbf{SpaCy:} TODO


\textbf{Stanza:} This toolkit implements full neural pipelines for processing raw text, including tagging morphological features using bidirectional long short-term memory networks and lemmatizing an ensemble of dictionary based and seq2seq methods \cite{qi2020stanza}. Because each step of the pipeline depends on the output of the previous step, in order to use Stanza for morphological tagging and lemmatization, we also use its tokenization and sentence-splitting. We use the Stanza model trained on the SynTagRus treebank\footnote{\url{https://universaldependencies.org/treebanks/ru_syntagrus/index.html}}. Unlike Mystem, Stanza always produces a single lemma and morphological slot, the disambiguation step is included within the model.

\subsection{Evaluation metrics}
%TODO: rewrite this section
When it comes to topic modeling on Russian, we would like to quantify the trade-offs between topic interpretability and loss of information that is linked to a surface form's morphology. We can apply Mystem or Stanza to retrieve the most likely morphological analysis for a surface form $w$ in the vocabulary $V$ to find a lemma $\ell_w$ and slot $\sigma_w$. Using the token-level topic assignments from Gibbs Sampling as our surface form $w$, we follow Thompson and Mimno (2018) in viewing single topic assignments for each surface form as a data table with columns: surface form $w$, topic assignment $k$, slot $\sigma$, lemma $\ell$. For a given topic $k$, we obtain the joint count of the slots for the topic $N(\sigma, k)$, the counts of the lemmas for a topic $N(\ell, k)$ and the marginal count variable for a topic $N(k)$. Also note that $\argmax_{w \in V} N(w, k)$ denotes the top key words or surface forms for the topic.

\subsubsection{Entropy-based morphology measurements}
% Ranges for these values will differ depending on the size of a language's morphological paradigms

\textbf{Morphological slot entropy:} The goal of this metric is to measure the concentration of slots within a given topic, a proxy for the enumerative complexity of the topic. Does a topic have a concentration of only a few morphological features or does it have a wide spread of the language's inventory of features? This metric is similar to Author Entropy discussed in Thompson and Mimno (2018), where the morphology of the language is the metadata we are attempting to capture, rather than the author of a document \cite{Thompson2018AuthorlessTM}. Topics that have low slot entropy would contain wordforms with the same grammatical features, for example different verbs conjugated in the first-person singular form or nominative case masculine nouns.
\begin{flalign}
    H(\sigma|k) &= \sum_\sigma P(\sigma|k) \log_2 P(\sigma|k) \\ \nonumber&= \sum_\sigma \frac{N(\sigma, k)}{N(k)} \log_2 \frac{N(\sigma, k)}{N(k)}
\end{flalign}

\textbf{Lemma entropy:} Similarly, we may want to know when a topic is dominated by a single lexeme, containing many grammatical forms of a single lexeme, but few other lexemes. For example, a topic may have many counts of different surface forms for each declension of a particular noun, its nominative, accustive, dative, etc... forms or even high counts for a single surface form, but relatively low counts of surface forms for any other lemma. Topics with very low lemma entropy may not be particularly useful to end users, as they reflect lexical and grammatical information known to every speaker of the language, but may not provide specific information about the corpus, other than the presence of a particular lexeme.
\begin{flalign}
    H(\ell|k) &= \sum_\ell P(\ell|k) \log_2 P(\ell|k) \\ \nonumber&= \sum_\ell \frac{N(\ell, k)}{N(k)} \log_2 \frac{N(\ell, k)}{N(k)}
\end{flalign}

\subsubsection{Counting-based morphology measurements}
In practice, topics are often identified by keywords, the most frequently allocated terms to a topic. However, without pre-processing it's possible that the set of top $n$ keywords consists of many surface forms of the same lexeme, obscuring forms of other lexemes that could be useful to identifying the topic. Similarly, it's possible that a lexeme's allocations to a topic are spread across many word forms, such that no forms of the lexeme appear in the keywords for the topic, even though this may be the most frequent lemma allocated for the topic. Both of these problems occurring simultaneously for many topics would suggest a need for post-processing treatment.

\textbf{Lemmas expressed by top $n$ key terms:} This set is targeted at understanding how concise the presentation of a topic's key terms is to a user. When its size is close to $n$, each key term presented to the user represents a unique lexeme or multiple lexemes in cases of lexical ambiguity. If the set's size is closer to 1, different forms of the same lexeme are repeated in the keywords.
\begin{flalign}
    K_\ell(k) &= \{\ell_w | w \in \{n \, \mathrm{largest} N(w, k)\}\}
\end{flalign}

\textbf{Top $n$ lemmas:} A topic's most frequent lexemes may not always overlap with the lexemes of its most frequent surface forms. Comparing the differences between the most a topic's most frequent lexemes, $L(k)$, defined below, and $K_\ell(k)$, will reveal topics where morphology impacts interpretability.
\begin{flalign}
    L(k) &= \{\ell | \ell \in \{n \, \mathrm{largest} N(\ell, k)\}\}
\end{flalign}

\textbf{Exclusivity:} Exclusivity quantifies the relative uniqueness of the top keywords in a topic. It is high when the terms most frequently generated by a topic are rarely generated by other topics in the model \cite{bischof2012exclusivity}. This metric can also be modified to quantify the relative uniqueness of lemmas to a topic, which we call \textit{lemma exclusivity}.

\subsubsection{Strength of treatment measurements}
These measurements quantify the aggressiveness of stemming or lemmatization.

\textbf{Type-token ratio:} Following Schofield and Mimno (2016), this corpus-level metric measures a stemmer or lemmatizer's conflation strength. It is found by taking the ratio of the number of word-type equivalence classes produced by the treatment (the post-treatment vocabulary size $|V|$) to the token counts for the corpus \cite{schofield-mimno-2016-comparing}.

\textbf{Character-token ratio:} This metric, also from Schofield and Mimno (2016), measures the aggressiveness of stemmers in trimming surface forms to a root form. It measures the average length of the tokens in the corpus after the stemming treatment. Because lemmatizers map surface forms to a normalized lemma instead, this metric isn't as meaningful for lemmatization.

\subsubsection{Topic Quality}
% TODO


\section{Results}
\subsection{Counting top lexemes}

\begin{figure*}[t]
    \label{fig:intersection}
    \captionof{figure}{Comparison of the top 20 lemmas for each topic and the lemmas covered by the topic's top 20 key terms, over 10 experiments on the untreated corpus. The cells show the number of topics with the corresponding set differences between $L(k)$ and $K_\ell(k)$. Values in the upper left indicate topics with high overlap in lemmas of the top terms and the topic's most frequent lemmas. Values in the lower left are topics where the top terms are obscured by the top terms, good candidates for post-stemming. Values in the lower right indicate large mismatches between those sets, a challenge for topic interpretability. }
    \begin{tabular}{ll}
        \includegraphics[width=0.5\textwidth]{tiger_intersections_50_topics.png} & \includegraphics[width=0.5\textwidth]{tiger_intersections_100_topics.png} \\
        \includegraphics[width=0.5\textwidth]{rnc_intersections_50_topics.png} & \includegraphics[width=0.5\textwidth]{rnc_intersections_100_topics.png}
    \end{tabular}
\end{figure*}


\subsection{VOI and topic stability}

\begin{figure*}[t]
    \label{fig:voi}
    \captionof{figure}{Variation of information between pre-processing treatments averaged over pairwise comparison of 10 experiments for each treatment. `Raw' indicates that no stemming or lemmatization pre-processing was performed.}
    \begin{tabular}{ll}
        \includegraphics[width=0.5\textwidth]{tiger_voi_50_topics.png} & \includegraphics[width=0.5\textwidth]{tiger_voi_100_topics.png} \\
        \includegraphics[width=0.5\textwidth]{rnc_voi_50_topics.png} & \includegraphics[width=0.5\textwidth]{rnc_voi_100_topics.png}
    \end{tabular}
\end{figure*}


\section{Future Work}
% Measurements on an agglutinative language


\bibliographystyle{acl_natbib}
\bibliography{references}


\begin{table*}
    \label{sec:poststem_topics}
    \captionof{table}{These are sample topics from 50 topic models trained on the untreated corpora demonstrating how the difference between $K_\ell(k)$ and $L(k)$ can be used to identify topics as candidates for post-stemming.}
    \begin{tabularx}{\textwidth}{|X|X|l|l|X|} \hline
    $K_\ell(k)$ & $L(k)$ & $|L(k) - K_\ell(k)|$ & $|K_\ell(k) - L(k)|$ &  \textbf{Comment} \\ \hline
    \end{tabularx}
\end{table*}

\end{document}
