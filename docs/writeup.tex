%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl}

\usepackage{times}
\usepackage{amsmath}
\usepackage{url}
\usepackage{latexsym}
\usepackage{hyperref}
\usepackage[pdftex]{graphicx}
\usepackage{makecell}
%\usepackage[T2A,T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian,english]{babel}
\usepackage{geometry}
\usepackage{pdflscape}
\usepackage{caption}
%\usepackage{showframe}
\renewcommand{\UrlFont}{\ttfamily\small}
\newcommand{\argmax}{\mathrm{argmax}}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{A Brother Karamazov: Quantifying Morphology in Topic Modeling of Literary Russian}

\author{Virginia Partridge \\
  University of Massachusetts Amherst\\
  \texttt{vcpartridge@umass.edu}
}
\date{}

\begin{document}
\maketitle
\begin{abstract}
TODO
\end{abstract}

\section{Introduction}
Latent Dirichlet Analysis (LDA) is a widely adopted approach for unsupervised topic modeling and has been used across disciplines for exploring themes and trends in large document collections. LDA has been applied to explore the ever-growing variety of text from online platforms and social media and to analyze language changes in academic fields over time \cite{koltsova2013,mcfarland2013differentiating, vogel-jurafsky-2012-said, mitrofanova2015probabilistic}. Assuming a bag-of-words approach, LDA produces latent topics as multinomial distributions over words and each topic is viewed as being generated by a mixture of topics \cite{blei2003,steyvers2007probabilistic}.

However, what happens when words in this bag-of-words approach are themselves are complex? We turn to topic modeling on Russian, a flective language with rich paradigms for nouns, adjectives and verbs \cite{wade2020comprehensive}. Russian's inflectional morphology increases the sparsity of words' surface forms in the collection, but it's unclear to what extent this sparsity impacts the interpretability and usefulness of topics. Stemming and lemmatization treatments are typical text preprocessing steps for topic modeling, even for English, which has relatively little inflectional morphology, but there is a lack of empirical evidence that these treatments improve the models from the perspective of human interpretability or quantitative measures of topic quality \cite{schofield-mimno-2016-comparing}.

Furthermore, conflation of surface word forms may mask phenomena of interest to researchers. Topic modeling is a popular tool for exploring gender bias in corpora \cite{vogel-jurafsky-2012-said,devinney-etal-2020-semi}, and many languages, Russian included, have inflectional morphology that marks gender. By normalizing tokens to a single form, topics learned in LDA won't distinguish between Russian's feminine, masculine and neuter word forms, which may or may not be desirable depending on the domain and researchers' goals. The situation in Russian is even more nuanced that the oft cited English example ``apple'', the company, as opposed to ``apples'', the fruit \cite{schofield-mimno-2016-comparing}, as the different surface forms in Russian do share an underlying lexical word sense, but their variation results from requirements of the language's syntax.

In this work we explore baseline performance of LDA for topic modeling on a Russian literary corpus and report both quantitatively and qualitatively on the resulting topics. We first establish that topic modeling in Russian behaves similarly to English in terms of correlation with corpus metadata, regardless of the stemming or lemmatization approach. By investigating topic models produced with no morphological preprocessing step, we demonstrate that morphological features can be evident in a topic and propose ways quantify this relationship. These observations cast doubt on whether preprocessing is necessary or if its use can actually obscure information. We also present post-processing of topics as an alternative to aid with interpretability by the end users. Finally, we compare various stemming and lemmatization treatments as preprocessing the corpus and contrast this with post-processing the keywords learned by models.

\section{Related Work}
Probabilistic topic modeling has been applied on Russian text data from academic fields, social media, and Wikipedia articles \cite{mitrofanova2015probabilistic,koltsova2013,May2016AnAO}. Prior to the work on Wikipedia, little attention was given to the role of lemmatization on topic modeling in Russian, and corpora were lemmatized by default. In studying Russian Wikipedia, May et al. (2016) address the impact of lemmatization on topic interpretability via a word intrusion evaluation task, finding that lemmatization may be beneficial. However, they also suggest measuring the effects of lemmatization and do not rule out that lemmatizing in post-processing would also be effective.

The proposal for applying stemming in post-processing comes from work comparing the effects of various stemming approaches on English \cite{schofield-mimno-2016-comparing}. After comparing the relative strengths, qualitative and quantitative impacts of rule-based and context-based stemmers for English, it was concluded that stemmers do not emprically improve LDA topic models and may even hurt topic stability. Post-processing still adds value from the perspective of topic interpretability, avoiding repeating surface forms of the same lexeme in topics' key word lists and presenting users with concise results.


\section{Methods}
\subsection{Framework for Morphological Complexity}
We will first clarify terms for discussing Russian's morphological paradigms, following frameworks for quantifying morphological complexity used in linguistics and computational linguistics \cite{baerman2015intro, cotterell-etal-2019-complexity}. We draw a distinction between \textit{derivational} morphology, the process by which new words are formed through changing meaning or part of speech, and \textit{inflectional} morphology, which can be simplistically understood as verb paradigms to capture subject-verb agreement or noun declensions for case and grammatical gender. For our purposes here, we are primarily interested in the equivalence classes formed by normalizing inflectional morphology, to use an English example, conflating ``respond" and ``responds", rather than ``respond" and ``responsiveness", although aggressive stemming methods will do both conflations.

In the word-based morphology framework, inflection is captured by triples consisting of the surface form (also called wordform) $w$, a lexeme  signifying the pairing of the surface for with a meaning and a slot $\sigma$, which can be understood as a set of ``atomic" units of morphological meaning, also called inflectional features \cite{aronoff1976word,sylak-glassman-etal-2015-language,cotterell-etal-2019-complexity}.
A lemma is the surface form used to look up the lexeme in a dictionary, such as the infinitive verb form. Measurements of the size of lexemes morphological paradigm capture \textit{enumerative complexity}, the number of distinct surface forms for a particular part-of-speech \cite{cotterell-etal-2019-complexity}. A lexeme's mapping between slots and the surface forms is not always straightforward, as multiple slots may be realized with a single surface form.  This type of morphological complexity is called \textit{syncretism} and is common in Russian noun and adjective declensions \cite{baerman2015understanding,Milizia2015PatternsOS}.

There are two morphological tagsets commonly used for natural language processing of Russian. The first originates with Zalizniak's Grammatical dictionary and is used in the Russian National Corpus. A detailed explanation of these tags can be found on the website of the Russian National Corpus\footnote{\url{https://ruscorpora.ru/new/en/corpora-morph.html}}. The second is the Universal Dependency tagset, which has been applied to the Russian dependency treebank SynTagRus \cite{Sharoff2011ThePP,lipenkova-soucek-2014-converting,mcdonald-etal-2013-universal}. Both of these tagsets express slots as a list of grammatical features and the two tagsets can be mapped to each other, although, as we shall see, their conflation classes (assigned lemmas) for Russian verbs differ significantly.

\subsection{Stemmers and Lemmatization Treatments}
\label{sec:stemmers}
Following Schofield and Mimno (2016), we distinguish between rule-based stemmers, which are deterministic, but only remove endings and do not map to lemmas, and context-based lemmatizers, which rely on a dictionary of word forms paired with outputs from a part-of-speech tagger to produce lemmas \cite{schofield-mimno-2016-comparing,Sharoff2011ThePP}. Rule-based methods make no distinction between inflectional and derivational morphological processes, leading to word types, conflation classes of terms whose original surface forms may cover several lemmas.

\textbf{Truncation:} This simple baseline method trims surface forms to the first $n$ characters \cite{schofield-mimno-2016-comparing}. We truncate with $n=5$.

\textbf{Snowball Stemmer:} This stemmer was introduced as a rigorous framework for implementing stemming algorithms for a variety of languages. We utilize the NLTK implementation\footnote{\url{https://www.nltk.org/api/nltk.stem.html}} with the original rules for Russian\footnote{\url{http://snowball.tartarus.org/algorithms/russian/stemmer.html}} \cite{snowball}.


\textbf{Mystem:} This Yandex-owned tool is the most popular lemmatizer for Russian and can be used with or without part-of-speech tags. Pairing a finite state machine algorithm for stemming with the Zalizniak Grammatical dictionary for morphological tags, this system outputs a list of possible lemmas and slots for a given token input. The system also produces probabilities for each lemma and slot based on word frequency statistics, although the source corpus for these probabilities is not clear \cite{Segalovich2003AFM}. This is not truly a context-based lemmatizer, as it does not use part-of-speech tags to disambiguate between lemmas or to assign a single slot to a syncretic surface form, but the word frequencies do represent some kind of contextual prior. We use the python wrapper for Mystem, pymystem3\footnote{\url{pythonhosted.org/pymystem3/pymystem3.html}}.


\textbf{Stanza:} This toolkit implements full neural pipelines for processing raw text, including tagging morphological features using bidirectional long short-term memory networks and lemmatizing an ensemble of dictionary based and seq2seq methods \cite{qi2020stanza}. Because each step of the pipeline depends on the output of the previous step, in order to use Stanza for morphological tagging and lemmatization, we also use its tokenization and sentence-splitting. We use the Stanza model trained on the SynTagRus treebank\footnote{\url{https://universaldependencies.org/treebanks/ru_syntagrus/index.html}}. Unlike Mystem, Stanza always produces a single lemma and morphological slot, the disambiguation step is included within the model.

Other well-known lemmatizers for Russian include TreeTagger, CSTLemmatiser, pymorphy2 \cite{May2016AnAO,Sharoff2011ThePP,pymorphy2}. We opted not to use these here due to time constraints, as they are either difficult to install (TreeTagger, CSTLemmatiser) or very slow (pymorphy2).

\subsection{Latent Dirichlet Analysis}
Latent Dirichlet Analysis uses the observed frequencies of vocabulary terms within documents to infer the \textit{latent}, or hidden, distributions of topics over words and topic assignments for each document. Once a number of topics $T$ is selected, the multinomial distributions $\phi_1,...\phi_T$ define the distribution of each topic $t$ over the vocabulary terms. Each $\phi_t$ is drawn from with a Dirichlet prior with concentration parameter $\beta$. Each document $d$ also has a multinomial distribution $\theta_d$ over the terms in the vocabulary, also drawn from a Dirichlet prior with concentration parameter $\alpha$. Viewing LDA as a generative process, taking a joint distribution of the observed and latent variables, you are finding the $\phi_t$ and $\theta_d$ that maximize the likelihood of the corpus if you were to assign tokens to documents using the marginal distributions over topic assignments for the terms in each document. Gibbs Sampling allows estimation of the posterior for the joint topic distribution conditioned on the observed term frequencies by directly assigning topics to each token in the corpus, iteratively sampling topics and updating topic assignments.
 \cite{steyvers2007probabilistic, blei2003,schofield-mimno-2016-comparing}.

Following Wallach et. al (2002), we will use a symmetric prior for $\beta$ and an asymmetric prior for $\alpha$ with the MALLET's Gibbs Sampling implementation to train topic models \cite{wallach2009rethinking,McCallumMALLET}. These parameters are optimized every 20 iterations after the first 50, the burn-in period. The Gibbs sampling implementation in MALLET allows us to directly inspect the topic assignments at the level of each token in a document.


\subsection{Evaluation metrics}
When it comes to topic modeling on Russian, we would like to quantify the trade-offs between topic interpretability and loss of information that is linked to a surface form's morphology. We can apply Mystem or Stanza to retrieve the most likely morphological analysis for a surface form $w$ in the vocabulary $V$ to obtain and lemma $\ell_w$ and slot $\sigma_w$. Using the token-level topic assignments from Gibbs Sampling as our surface form $w$, we follow Thompson and Mimno (2018) in viewing single topic assignments for each surface form as a data table with columns: surface form $w$, topic assignment $z$, slot $\sigma$, lemma $\ell$. For a given topic $k$, we obtain the joint count of the slots for the topic $N(\sigma, k)$, the counts of the lemmas for a topic $N(\ell, k)$ and the marginal count variable for a topic $N(k)$. Also note that $\argmax_{w, \in V} N(w, k)$ denotes the top key words or surface forms for the topic.

\textbf{Morphological slot entropy:} The goal of this metric is to measure the concentration of slots within a given topic, a proxy for the enumerative complexity of the topic. Does a topic have a concentration of only a few morphological features or does it have a wide spread of the language's inventory of features? This metric is similar to Author Entropy discussed in Thompson and Mimno (2018), where the morphology of the language is the metadata we are attempting to capture, rather than the author of a document \cite{Thompson2018AuthorlessTM}. Topics that have low slot entropy would contain lemmas with the same grammatical features, for example different verbs conjugated in the first-person singular form or nominative case masculine nouns.
\begin{flalign}
    H(\sigma|k) &= \sum_\sigma P(\sigma|k) \log_2 P(\sigma|k) \\ \nonumber&= \sum_\sigma \frac{(N(\sigma, k))}{N(k)} \log_2 \frac{(N(\sigma, k))}{N(k)}
\end{flalign}

\textbf{Lemma entropy:} Similarly, we may want to know when a topic is dominated by a single lexeme, containing many grammatical forms of a single lexeme, but few other lexemes. For example, a topic may have many counts of different surface forms for each declension of a particular noun, its nominative, accustive, dative, etc... forms or even high counts for a single surface form, but relatively low counts of surface forms for any other lemma. Topics with very low lemma entropy may not be particularly useful to end users, as they reflect lexical and grammatical information known to every speaker of the language, but may not provide specific information about the corpus, other than the presence of a particular lexeme.
\begin{flalign}
    H(\ell|k) &= \sum_\ell P(\ell|k) \log_2 P(\ell|k) \\ \nonumber&= \sum_\ell \frac{(N(\ell, k))}{N(k)} \log_2 \frac{(N(\ell, k))}{N(k)}
\end{flalign}

\textbf{Ratio of slots to top $n$ key terms:} Here we are capturing whether a topic makes a particular grammatical feature obvious when results are displayed to the user. Although this doesn't account for the case when paradigms are syncretic on different dimensions, for example comparing first declension to third declension nouns \cite{wade2020comprehensive}, when this value is low, it suggests that the user will notice some sort of grammatical pattern within the topics.
\begin{flalign}
    R_\sigma(k) &= \frac{|\{\sigma_w | w \in  \{n \, \mathrm{largest} N(w, k)\} \}|}{n}
\end{flalign}


\textbf{Ratio of lemmas to top $n$ key terms:} This metric is targeted at understanding how concise the presentation of a topic's key terms is to a user. When the value is close to 1, each surface form presented to the user represents a unique lexeme. When this value is low, different forms of the same lexeme are repeated, the situation that has motivated the use for lemmatization or stemming to begin with.
\begin{flalign}
    R_\ell(k) &= \frac{|\{\ell_w | w \in \{n \, \mathrm{largest} N(w, k)\}\}|}{n}
\end{flalign}

\textbf{Type-token ratio:} Following Schofield and Mimno (2016), this corpus-level metric measures a stemmer or lemmatizer's conflation strength. It is found by taking the ratio of the number of word-type equivalence classes produced by the treatment, in other words the post-treatment vocabulary size $|V|$, to the token counts for the corpus \cite{schofield-mimno-2016-comparing}.

\textbf{Character-token ratio:} This metric, also from Schofield and Mimno (2016), measures the aggressiveness of stemmers in trimming surface forms to a root form. It measures the average length of the tokens in the corpus after the stemming treatment. Because lemmatizers map surface forms to a normalized lemma instead, this metric isn't as meaningful for lemmatization.

\textbf{Author Entropy:} Determining whether the topics correlate with known metadata provides a sanity check on the models. This metric, introduced by Thompson and Mimno  (2018) measures how evenly a topic's tokens are spread across authors. We should expect to see both author-specific topic and general topics that are common to many authors \cite{Thompson2018AuthorlessTM}.

\textbf{Coherence:} We additionally report coherence, a measure of topic quality that relies on document co-occurence frequencies of word types, which has been reported to agree with human evaluations of topic quality \cite{mimno2011optimizing}. However, further work is required to adjust this measurement for the smaller vocabulary sizes resulting from conflation treatments \cite{schofield-mimno-2016-comparing}. The coherence results reported here cannot be interpreted as evidence for lemmatization helping or hurting the models' quality.

\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{\begin{tabular}{|l|l|l|l|l|l|}
    \hline
    \thead{\textbf{Stemming/Lemmatization}\\\textbf{Treatment}} & \thead{\textbf{Unpruned number}\\\textbf{of tokens}} & \thead{\textbf{Unpruned} \\\textbf{vocabulary size}} & \thead{\textbf{Number of tokens}\\\textbf{after pruning}} & \thead{\textbf{Vocabulry size}\\\textbf{after pruning}} & \thead{\textbf{Processing time for}\\\textbf{treatment (minutes)}}\\\hline
    No treatment & 6081210 & 319459 & 3321349 & 80540 &-\\\hline
Pymystem3 & 6084073 & 80163 & 2976804 & 32648 & 4.7 \\\hline
Snowball & 6081203 & 108533 & 3070870 & 35938 & 5 \\\hline
Stanza & 6097070 & 119602 & 2980649 & 38435 & 211.2 \\\hline
Truncate to 5 & 6081210 & 59469& 3357584 & 27994 & 0.25 \\\hline
    \end{tabular}}
    \caption{Token level corpus statistics for each lemmatization and stemming treatment. Differences in unpruned token counts are due to Pymystem3 and Stanza using their own tokenization and Snowball normalizing some single characters to empty strings.}
    \label{table:corpus}
\end{table*}

\begin{figure*}[t]
    \includegraphics[width=0.5\textwidth]{document_count_by_author.png}
    \includegraphics[width=0.5\textwidth]{token_count_by_author.png}
    \caption{The number of documents per author used to train topic models (left) and the token counts by author before pruning (right).}
    \label{fig:docsByAuthor}
\end{figure*}


\section{Corpus}
The selected RussianNovels\footnote{\url{https://github.com/JoannaBy/RussianNovels}} corpus is a collection of 101 Russian literary works from the 19th and 20th centuries by 23 authors. The collection primarily consists of novels and novellas, but there are some plays (Chekhov and Sologub) and short stories (Gogol) as well. Duplicated works and multiple versions of the same work by different translators were removed. The deduplicated version of the corpus is available on Github \footnote{\url{https://github.com/ginic/RussianNovels/tree/cleanups}} with a change log.

Each work was subdivided into passages at least 500 tokens long, where tokens are determined by a simple non-whitespace pattern. This resulted in a corpus of 10,305 documents, broken down by author in figure \ref{fig:docsByAuthor}. Next, the corpus was re-tokenized using a regular expression capturing strings of alphabet characters of any length, with punctuation allowed between alphabet characters. Token-level statistics are given in table \ref{table:corpus}. We produced separate versions of the corpus for each treatment described in section \ref{sec:stemmers}. After conflation treatments, the resulting terms were pruned to a maximum document frequency of 25\% of the corpus and minimum term frequency of 5 occurences in the entire corpus. We chose these pruning settings as a reasonable alternative to manually determining a stopword list, as that process can be challenging and subjective \cite{schofield-etal-2017-pulling}.

\section{Results}
After pre-processing the corpus using each stemmer or lemmatizer, we trained 5 models for each number of topics $T\in \{50, 100, 250, 500\}$. We also train models on the corpus with no pre-processing treatment at all, other than pruning. Additionally, we did post-lemmatize the models without conflation treatment, using Mystem to produce topics with lemmas as the associated terms rather than surface-forms, but time did not allow for in depth investigation into these models.

\begin{figure*}[t]
    \includegraphics[width=0.5\textwidth]{pruned_type_token_ratio.png}
    \includegraphics[width=0.5\textwidth]{pruned_character_token_ratio.png}
    \caption{The ratio of word types to tokens for each conflation treatment after the vocabulary is pruned is shown left and demonstrates the strength of conflation method in reducing the vocabulary. Character to token ratio shows lemmatization returns longer normalized strings, while stemming shortens them.}
    \label{fig:conflation_merics}
\end{figure*}

\begin{figure*}[t]
    \includegraphics[width=0.5\textwidth]{unweighted_slot_entropy.png}
    \includegraphics[width=0.5\textwidth]{unweighted_lemma_entropy.png}
    \includegraphics[width=0.5\textwidth]{slots_to_top_20_surface_forms.png}
    \includegraphics[width=0.5\textwidth]{lemmas_to_top_20_surface_forms.png}
    \caption{Values of metrics that capture morphological information in topics. These results are for topic models trained on the untreated corpus.}
    \label{fig:slot_lemma_figs}
\end{figure*}
\subsection{Qualitative Observations}
At the most basic level, we want to check to what extent topic modeling on this Russian literary corpus produces sensible results, showing some topics that correlate with metadata and others that capture general themes not specific to a particular author or time period. Author entropy does not directly measure model quality, in fact users often want topic models not correlated with known metadata \cite{Thompson2018AuthorlessTM}. However, it can at least reassure us that LDA performs on Russian in a way that is expectedly similar to English. Both the author correlation measures plotted in Appendix \ref{sec:author} and anecdotal analysis of topics' key words find topics with both low and high author entropy.

Moreover, lemmatization and stemming with Snowball do not seem to affect whether author-specific topics or cross-cutting topics are learned, regardless of the number of topics. Compared to the other conflation treatments, truncation does reduce the number of topics with low author entropy, but only when the number of topics is low.

\subsection{Lemmatizer Choice Matters}

When examining the relative strengths of the stemmers and lemmatizers, laid out in figure \ref{fig:conflation_merics}, functional differences between the two lemmatizers are revealed. As expected, truncation, having the least type-token ratio, is the most aggressive treatment in terms of producing the largest word-type equivalence classes. The suprise is that Mystem is the next most agressive, followed by Snowball, then Stanza. Since both Mystem and Stanza map surface forms to a normalized dictionary lemma, we expect them to be roughly equivalent in conflation strength.

This seemingly counter-intuitive result exposes a difference in the way that Mystem and Stanza handle verbal aspect. Nearly all Russian verbs have two infinitive forms, one for the imperfective aspect and one for the perfective aspect \cite{wade2020comprehensive}. Mystem treats all conjugations of the both imperfective and perfective aspects as surface forms of the same lexeme, mapping to the imperfective inifitive as the lemma. In contrast, Stanza maps to separate lemmas, imperfective forms to the imperfective infintive and perfective forms to the perfective infinitive. A clarifying example is given in table \ref{table:verbaspect}. As a non-native speaker, it's difficult predict what impact this distinction would have on topic interpretability, but the takeaway is that not all lemmatizers are equal. The choice of lemmatizer is not obvious and requires consideration of how the implementation groups the language's grammatical features and which grammatical feature matter in the corpus domain.

\begin{center}
    \includegraphics[width=0.5\textwidth]{lemma_slot_entropy_scatter.png}
    \captionof{figure}{Lemma entropy vs slot entropy for models without lemmatization.}
    \label{fig:lemma_vs_slot_entropy}
\end{center}



\section{Future Work}
Standardization of stop words for more consistent comparison between metrics
Shannon-Jenson divergence
Manage vocabularies between stemmed and unstemmed corpus, account for vocabulary size when producing metrics, especially coherence
Stability


Corpora from other domains - Russian National Corpus and OpenCorpus

\bibliographystyle{acl_natbib}
\bibliography{references}

\appendix
\onecolumn
\section{Author Correlation Metrics with Conflation Treatments}
\label{sec:author}
\begin{center}
    \includegraphics[width=\textwidth]{AuthorEntropy.png}
    %\includegraphics[width=\textwidth]{BalancedAnthors.png}
    %\includegraphics[width=\textwidth]{MinusMajorAuthor.png}
    \captionof{figure}{Author Entropy metric values for the 5 topic models trained for each number of topics $T \in \{50, 100,250, 500\}$, broken down by the number of topics and the type of conflation treament used.}
    \label{fig:authorentropy}
\end{center}
\begin{center}
    \includegraphics[width=\textwidth]{BalancedAuthors.png}
    %\includegraphics[width=\textwidth]{MinusMajorAuthor.png}
    \captionof{figure}{Balanced Author metric values for the 5 topic models trained for each number of topics $T \in \{50, 100,250, 500\}$, broken down by the number of topics and the type of conflation treament used.}
    \label{fig:balauthor}
\end{center}
\begin{center}
    \includegraphics[width=\textwidth]{MinusMajorAuthor.png}
    \captionof{figure}{Balanced Author metric values for the 5 topic models trained for each number of topics $T \in \{50, 100,250, 500\}$, broken down by the number of topics and the type of metrics  conflation treament used.}
    \label{fig:mmauthor}
\end{center}
\pagebreak
\section{MALLET Diagnostics Metrics}
\begin{figure*}[!th]
    \includegraphics[width=0.5\textwidth]{negative_coherence.png}
    \includegraphics[width=0.5\textwidth]{document_entropy.png}
    \includegraphics[width=0.5\textwidth]{uniform_dist.png}
    \includegraphics[width=0.5\textwidth]{corpus_dist.png}
    \includegraphics[width=0.5\textwidth]{exclusivity.png}
    \includegraphics[width=0.5\textwidth]{word-length.png}
    \caption{Comparing averages over of metrics produced by MALLET between conflation treatments. These are mainly provided as a sanity check. Note that word-length is much higher for lemmatizers than stemmers, as expected.}
    \label{fig:slot_lemma_figs}
\end{figure*}


\begin{landscape}
\thispagestyle{empty}
\newgeometry{left=1.5cm,top=1.5cm,textwidth=27cm,textheight=16.0cm, headheight=0.17573cm,headsep=0cm}
\onecolumn
\section{Outputs of Lemmatizers and Stemmers}
\label{sec:appendix_morph}
\begin{center}
%\begin{table*}[th]
%    \centering
    \resizebox{\textwidth}{!}{\begin{tabular}{|l|l|l|l|l|}
    \hline
    \thead{\textbf{Surface}\\\textbf{form}} & \thead{\textbf{Translation and}\\\textbf{morphological features}} & \textbf{Mystem} & \textbf{Stanza} & \textbf{Snowball}\\\hline
    \foreignlanguage{russian}{ответ} & \thead{`answer' \\ Noun, Masculine, Singular, Inanimate \\Nominative or Accusative case} & \thead{\foreignlanguage{russian}{ответ}\\ Noun,Masc,Inan\\(Acc,Sing\textbar Nom,Sing)} & \thead{\foreignlanguage{russian}{ответ} \\Noun\\Animacy=Inan, Case=Nom, Gender=Masc, Number=Sing} & \foreignlanguage{russian}{ответ}\\\hline
    \foreignlanguage{russian}{ответим} & \thead{`we will answer/have answered' \\ Intransitive Verb, Perfective, 1st person, Plural, Future tense\\ Indicative or Imperative mood} & \thead{\foreignlanguage{russian}{отвечать}\\Verb,Intransive\\(Pl,Imperative,1st Pers,Perf\textbar NonPast,Pl,Indicative,1st Pers,Perf)} & \thead{\foreignlanguage{russian}{ответить} \\Verb\\Aspect=Perf, Mood=Ind, Number=Plur, Person=1, Tense=Fut, VerbForm=Fin, Voice=Act,} & \foreignlanguage{russian}{ответ} \\\hline
    \foreignlanguage{russian}{отвечать} & \thead{`to answer' \\ Intransitive Verb, Imperfective, Infinitive} & \thead{\foreignlanguage{russian}{отвечать}\\Verb,Intransive\\Inf,Imp} & \thead{\foreignlanguage{russian}{отвечать} \\Verb\\Aspect=Imp, VerbForm=Inf, Voice=Act,} & \foreignlanguage{russian}{отвеча}\\\hline
    \foreignlanguage{russian}{ответить} & \thead{`to answer'\\Intransitive Verb, Perfective, Infinitive} & \thead{\foreignlanguage{russian}{отвечать}\\Verb,Intransive\\Inf,Perf} & \thead{\foreignlanguage{russian}{ответить} \\Verb\\Aspect=Perf, VerbForm=Inf, Voice=Act,} & \foreignlanguage{russian}{ответ}\\\hline
    \foreignlanguage{russian}{большой} & \thead{`big' \\Adjective, Sing\\Masc, Animate: Nominative\\
    Masc, Inanimate: Nominative, Accuastive\\Fem: Genitive, Prepositional, Dative, Instrumental} & \thead{\foreignlanguage{russian}{большой}\\Adjective\\(Acc,Sing,Full,Masc,Inan\textbar Acc,Sing,Full,Masc,Inan\textbar Nom,Sing,Full,Masc\textbar \\Prep,Sing,Full,Fem\textbar Dat,Sg,Full,Fem\textbar Gen,Sing,Full,Fem\textbar Instr,Sing,Full,Fem)'} & \thead{\foreignlanguage{russian}{большой} \\Adjective\\Case=Nom,Degree=Pos,Gender=Masc,Number=Sing} & \foreignlanguage{russian}{больш}\\\hline
    \end{tabular}}
    \captionof{table}{Contrasting lemmatization and stemming outputs for various surface forms. Observe that Mystem output captures syncretism and that Stanza and Mystem return different lemmas for perfective verbs. Note that Mystem outputs are translated from Russian abbreviations.}
    \label{table:verbaspect}

\end{center}
%\end{table*}


\end{landscape}



\end{document}
