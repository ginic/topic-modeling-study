%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl}

\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{hyperref}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Give me a title!}

\author{Virginia Partridge \\
  University of Massachusetts Amherst\\
  \texttt{vcpartridge@umass.edu}
}
\date{}

\begin{document}
\maketitle
\begin{abstract}
TODO
\end{abstract}

\section{Motivation}
Latent Dirichlet Analysis (LDA) is a widely adopted approach for unsupervised topic modeling and has been used across disciplines for exploring themes and trends in large document collections. LDA has been applied to explore the ever-growing variety of text from online platforms and social media and to analyze language changes in academic fields over time \cite{koltsova2013,mcfarland2013differentiating, vogel-jurafsky-2012-said, mitrofanova2015probabilistic}. Topic modeling can be used to
Assuming a bag-of-words approach, LDA produces latent topics as multinomial distributions over words and each topic is viewed as being generated by a mixture of topics \cite{blei2003,steyvers2007probabilistic}.

However, what happens when words in this bag-of-words approach are themselves are complex? Inflectional morphology increases the sparsity of words' surface forms in the collection, but it's unclear to what extent this sparsity impacts the interpretability and usefulness of topic models. Stemming and lemmatization treatments are typical text preprocessing steps for topic modeling, even for English, which has relatively little inflectional morphology, but there is a lack of empirical evidence that these treatments improve the models from the perspective of human interpretability or quantitative measures of topic quality \cite{schofield-mimno-2016-comparing}.

Furthermore, conflation of surface word forms may mask phenomena of interest to researchers. Topic modeling is a popular tool for exploring gender bias in corpora \cite{vogel-jurafsky-2012-said,devinney-etal-2020-semi}, and many languages, Russian included, have inflectional morphology that marks gender. By normalizing tokens to a single form, topics learned in LDA won't distinguish between Russian's feminine, masculine and neuter word forms, which may or may not be desirable, depending on the domain and researchers' goals.

In this work we explore baseline performance of LDA for topic modeling on a Russian literary corpus. We report both quantitatively and qualitatively on the resulting topics and investigate whether the learned topics demonstrate similar.  how a language's inflectional morphology can affect the topics learned in LDA. We propose some ways to quantify the role of morphology in topics.

\section{Related Work}

\section{Methods}
\subsection{Framework for discussing morphological complexity}
We would like to clarify terms for discussing Russian's morphological paradigms, following frameworks for quantifying morphological complexity used in linguistics and computational linguistics \cite{baerman2015intro, cotterell-etal-2019-complexity}. First, we must draw a distinction between \textit{derivational} morphology, the process by which new words are formed, and \textit{inflectional} morphology, which can be simplistically understood as verb paradigms to capture subject-verb agreement or noun declensions for case and grammatical gender. For our purposes here, we are primarily interested in the equivalence classes formed by normalizing inflectional morphology, for example conflating ``respond" and ``responds", rather than ``respond" and ``responsiveness", although more aggressive stemming methods will do both conflations.

In the word-based morphology framework, inflection is captured by triples consisting of the surface form (also called wordform) $w$, a lexeme signifying the pairing of the surface for with a meaning and a slot $\sigma$, which can be understood as a set of ``atomic" units of morphological meaning, also called inflectional features \cite{aronoff1976word,sylak-glassman-etal-2015-language,cotterell-etal-2019-complexity}.
A lemma is the surface form used to look up the lexeme in a dictionary, such as the infinitive verb form. The set of inflectional features used for Russian

%Russian has noun-adjective gender agreement and verbs have subject gender agreement in the past tense.


lemmas vs lexemes vs types, paradigms
tokens vs surface forms
slots
Russian as a flective language

\subsection{Latent Dirichlet Analysis}
symmetric vs asymmetric prior
mallet gibbs sampling implementation

\subsection{Evaluation metrics}

\section{Corpus}

\url{https://github.com/JoannaBy/RussianNovels}
\subsection{Conflation methods and vocabulary reduction}


\bibliographystyle{acl_natbib}
\bibliography{references}
\end{document}