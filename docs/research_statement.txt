Lemmatization and topic quality in LDA for morphologically complex languages:

Latent Dirichlet Analysis (LDA) is a popular unsupervised approach to discover topics in large document collections across a wide variety of fields. Recent work showed that stemming, a common text pre-proccesing step, has detrimental effects on topic models for English and argued in support of post-processing keywords for learned topics. Since English is less morphologically complex (has smaller inflectional paradigms) than many other languages, the trade-offs may be different when working with document collections for an inflective or agglutinative language. Indeed, our previous work on topic modeling for Russian found that without pre-processing LDA produced topics closely identified with particular grammatical features (e.g. verbs with only feminine or masculine subjects) and that post-processing obscures this grammatical information, making results less interpretable to users. In this work, we will analyze how stemming and lemmatization treatments affect the coherence and stability of topic models with varying numbers of topics for text collections in Russian and another synthetic language over multiple domains. Our goal is quantify the trade-offs in topic modeling on morphologically complex languages that influence topics' quality and interpretability to users.